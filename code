import nltk
from nltk.corpus import twitter_samples
import numpy as np
import matplotlib.pyplot as plt 
nltk.download('twitter_samples')
#select the set of positive and negative tweets
all_positive_tweets = twitter_samples.strings('positive_tweets.json')
all_negative_tweets = twitter_samples.strings('negative_tweets.json')
print('Number of positive tweets: ', len(all_positive_tweets))
print('Number of negative tweets: ', len(all_negative_tweets))

print('\nThe type of all_positive_tweets is: ', type(all_positive_tweets))
print('The type of a tweet entry is: ', type(all_negative_tweets[0]))
print("Positive Tweet Example:")
print(all_positive_tweets[0])

print("\nNegative Tweet Example:")
print(all_negative_tweets[0])
import re                   #library for regular expression operations
import string               #for string operations

from nltk.corpus import stopwords           #module  for stop words that come with NLTK
from nltk.stem import PorterStemmer         #module for stemming
from nltk.tokenize import TweetTokenizer   #module for tokenizing strings
def remove_hyperlinks_marks_styles(tweet):
    
    #remove old style retweet text 'RT'
    new_tweet = re.sub(r'^RT[\s]+', '', tweet)
    
    #remove hyperlinks
    new_tweet = re.sub(r'https?:\/\/.*[\r\n]*', '', new_tweet)
    
    #remove hashtags
    #only removing hash sign from the words
    new_tweet = re.sub(r'#', '', new_tweet)
    
    return new_tweet
    
#instantiate tokenizer class
tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,
                          reduce_len=True)

def tokenize_tweet(tweet):
    tweet_tokens = tokenizer.tokenize(tweet)
    
    return tweet_tokens
    
nltk.download('stopwords')
#Import the english stop words list from NLTK
stopwords_english = stopwords.words('english')

punctuations = string.punctuation

def remove_stopwords_punctuations(tweet_tokens):
    
    tweets_clean = []
    
    for word in tweet_tokens:
        if (word not in stopwords_english and word not in punctuations):
            tweets_clean.append(word)
            
    return tweets_clean
    
stemmer = PorterStemmer()

def get_stem(tweets_clean):
    tweets_stem = []
    
    for word in tweets_clean:
        stem_word = stemmer.stem(word)
        tweets_stem.append(stem_word)
        
    return tweets_stem
    
tweet_example = all_positive_tweets[2277]
print(tweet_example)

processed_tweet = remove_hyperlinks_marks_styles(tweet_example)
print("\nRemoved hyperlinks, Twitter marks and styles:")
print(processed_tweet)

tweet_tokens = tokenize_tweet(processed_tweet)
print("\nTokenize the string:")
print(tweet_tokens)

tweets_clean = remove_stopwords_punctuations(tweet_tokens)
print("\nRemove stopwords and punctuations:")
print(tweets_clean)

tweets_stem = get_stem(tweets_clean)
print("\nGet stem of each word:")
print(tweets_stem)

def process_tweet(tweet):
    
    processed_tweet = remove_hyperlinks_marks_styles(tweet)
    tweet_tokens = tokenize_tweet(processed_tweet)
    tweets_clean = remove_stopwords_punctuations(tweet_tokens)
    tweets_stem = get_stem(tweets_clean)
    
    return tweets_stem
    
tweet_example = all_negative_tweets[1000]
print(tweet_example)

processed_tweet = process_tweet(tweet_example)
print(processed_tweet)

test_pos = all_positive_tweets[4000:]
train_pos = all_positive_tweets[:4000]
test_neg = all_negative_tweets[4000:]
train_neg = all_negative_tweets[:4000]

train_x = train_pos + train_neg
test_x = test_pos + test_neg

train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))
test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))

def create_frequency(tweets, ys):
    freq_d = {}
    
    #Todo: create a frequency dictionary
    for tweet, y in zip(tweets, ys):
        for word in process_tweet(tweet):
            
            pair = (word, y)
            
            if pair in freq_d:
                freq_d[pair] += 1
                
            else: 
                freq_d[pair] = freq_d.get(pair, 1)
                
    return freq_d
#testing function
tweets = ['i am happy', 'i am tricked', 'i am sad', 'i am tired', 'i am tired']
ys = [1, 0, 0, 0, 0]

freq_d = create_frequency(tweets, ys)
print(freq_d)

freqs = create_frequency(train_x, train_y)
def train_naive_bayes(freqs, train_x, train_y):
    loglikelihood = {}
    logprior = 0
    
    #calculate the number of unique words in vocab
    unique_words = set([pair[0] for pair in freqs.keys()])
    V = len(unique_words)
    
    #calculate N_pos and N_neg
    N_pos = N_neg = 0
    for pair in freqs.keys():
        #Todo get N_pos and N_neg
        if pair[1] > 0:
            N_pos += freqs[(pair)]
            
        else:
            N_neg += freqs[(pair)]
            
    #Todo: calculate number of documents(tweets)
    D = train_y.shape[0]
    
    #Todo: calculate D_pos, the number of positive documents(tweets)
    D_pos = sum(train_y)
    
    #Todo: calculate D_neg, the number of negative documents(tweets)
    D_neg = D - sum(train_y)
    
    #Calculate logprior
    logprior = np.log(D_pos) - np.log(D_neg)
    
    #for each unique word
    for word in unique_words:
        #get pos and neg freq of the word
        freq_pos = freqs.get((word, 1), 0)
        freq_neg = freqs.get((word, 0), 0)
        
        #calc probability that the word is positive and negative
        p_w_pos = (freq_pos + 1) / (N_pos + V)
        p_w_neg = (freq_neg + 1) / (N_neg + V)
        
        #calculate the log likelihood of the word
        loglikelihood[word] = np.log(p_w_pos / p_w_neg)
        
    return logprior, loglikelihood
    
logprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)
print(logprior)
print(len(loglikelihood))

def naive_bayes_predict(tweet, logprior, loglikelihood):
    
    #process the tweet to get a list of words
    word_l = process_tweet(tweet)
    
    #initialize prob to zero
    p = 0
    
    #add logprior
    p += logprior
    
    for word in word_l:
        #get log likelihood of each keyword
        
        if word in loglikelihood:
            p += loglikelihood[word]
            
    return p
    
#test run to predict
for tweet in ['I am happy', 'I am so bad', 'this movie should have been bad', 'Today was such a tiring day', 'Weather is very unpleasant', 
             'Great great :)', 'bad bad bad', 'great great great']:
    #print('%s -> %f' % (tweet, naive_bayes_predict(tweet, logprior, loglikelihood)))
    p = naive_bayes_predict(tweet, logprior, loglikelihood)
    
    #print(f'{tweet} -> {p:.2f} ({p_category})')
    print(f'{tweet} -> {p:.2f}')
